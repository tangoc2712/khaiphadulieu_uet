{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13vWHH1Fsih0",
        "outputId": "743620cf-b18d-4e81-f463-1216c2ab53e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj_dcnv5sobb",
        "outputId": "66e5dd39-cfb6-403c-f936-40522368e0f0"
      },
      "source": [
        "!pip install emoji unidecode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.9MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=6801b10f5b33a4eceaa2fa31d76f238f55c549cedf92be75fd17a880ecf1d07d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, unidecode\n",
            "Successfully installed emoji-0.6.0 unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrgzUj6isw12",
        "outputId": "731380fa-3dd9-4a76-b75a-e632a89b9c40"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import MWETokenizer, word_tokenize, RegexpTokenizer\n",
        "import re\n",
        "import nltk\n",
        "import unicodedata\n",
        "\n",
        "multiple_punctuation_pattern = re.compile(r\"([\\\"\\.\\?\\!\\,\\:\\;\\-])(?:[\\\"\\.\\?\\!\\,\\:\\;\\-]){1,}\")\n",
        "word_tokenizer = MWETokenizer(separator='')\n",
        "multiple_emoji_pattern = re.compile(u\"([\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\u00a9\"\n",
        "        u\"\\u00ae\"\n",
        "        u\"\\u2000-\\u3300\"\n",
        "        \"]){1,}\", flags= re.UNICODE )\n",
        "\n",
        "normalizer = {'òa': 'oà',\n",
        "              'óa': 'oá',\n",
        "              'ỏa': 'oả',\n",
        "              'õa': 'oã',\n",
        "              'ọa': 'oạ',\n",
        "              'òe': 'oè',\n",
        "              'óe': 'oé',\n",
        "              'ỏe': 'oẻ',\n",
        "              'õe': 'oẽ',\n",
        "              'ọe': 'oẹ',\n",
        "              'ùy': 'uỳ',\n",
        "              'úy': 'uý',\n",
        "              'ủy': 'uỷ',\n",
        "              'ũy': 'uỹ',\n",
        "              'ụy': 'uỵ',\n",
        "              'Ủy': 'Uỷ'}\n",
        "correct_mapping = {\n",
        "      \"m\": \"mình\",\n",
        "      \"mik\": \"mình\",\n",
        "      \"ko\": \"không\",\n",
        "      \"k\": \" không \",\n",
        "      \"kh\": \"không\",\n",
        "      \"khong\": \"không\",\n",
        "      \"kg\": \"không\",\n",
        "      \"khg\": \"không\",\n",
        "      \"tl\": \"trả lời\",\n",
        "      \"r\": \"rồi\",\n",
        "      \"ok\": \"tốt\",\n",
        "      \"dc\": \"được\",\n",
        "      \"vs\": \"với\",\n",
        "      \"đt\": \"điện thoại\",\n",
        "      \"thjk\": \"thích\",\n",
        "      \"thik\": \"thích\",\n",
        "      \"qá\": \"quá\",\n",
        "      \"trể\": \"trễ\",\n",
        "      \"bgjo\": \"bao giờ\",\n",
        "      \"''\": '\"',\n",
        "      \"``\": '\"'\n",
        "}\n",
        "\n",
        "def normalize_text(text):\n",
        "  for absurd, normal in normalizer.items():\n",
        "    text = text.replace(absurd, normal)\n",
        "\n",
        "  # for l in vn_location:\n",
        "  #   text = text.replace(l, ' location ')\n",
        "\n",
        "  return text\n",
        "\n",
        "def tokmap(tok):\n",
        "  if tok.lower() in correct_mapping:\n",
        "      return correct_mapping[tok.lower()]\n",
        "  else:\n",
        "      return tok\n",
        "\n",
        "def preprocess(text):\n",
        "  global i\n",
        "  text = multiple_emoji_pattern.sub(r\"\\g<1> \", text) # \\g<1>\n",
        "  text = multiple_punctuation_pattern.sub(r\" \\g<1> \", text)\n",
        "  text = unicodedata.normalize(\"NFC\", text)\n",
        "  text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b(\\/)?', 'url', text)\n",
        "  text = re.sub(\"\\.\", \" . \", text)\n",
        "  text = re.sub(\"'\", \"' \", text)\n",
        "  text = re.sub('\"', '\" ', text)\n",
        "  text = re.sub('/', ' / ', text)\n",
        "  text = re.sub('-', ' - ', text)\n",
        "  text = re.sub(',', ' , ', text)\n",
        "  text = re.sub(r'\\s{2,}', ' ', text)\n",
        "  text = normalize_text(text)\n",
        "  # text = re.sub(r'\\#[^\\s]+', ' hastag ', text)\n",
        "  text = re.sub(r'(|\\s)([\\d]+k)(\\s|$)', ' cureency_k ', text)\n",
        "  text = re.sub(r'(([\\d]{2,4}\\s){2,}([\\d]+)?|(09|01|[2|6|8|9]|03)+([0-9]{8})\\b)', ' phone_number ', text)\n",
        "  text = re.sub(r'\\d', \"_digit\", text)\n",
        "  tokens = word_tokenizer.tokenize(word_tokenize(text))\n",
        "  tokens = list(map(tokmap, tokens))\n",
        "  # return tokens\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "print(preprocess('Thầy Trần Mai Vũ à một cổ động viên trung thành của đội bóng Manchester United'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Thầy Trần Mai Vũ à một cổ động viên trung thành của đội bóng Manchester United\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLMPdbwxs1tF",
        "outputId": "44aa21ae-f8f8-423a-eb05-cc5a80d8d011"
      },
      "source": [
        "with open('/content/drive/MyDrive/KPDL_1/sentiment_analysis_test_unlabel.v1.0.txt') as f:\n",
        "    sentiment_analysis_test = f.read().strip().split('\\n')\n",
        "    sentiment_analysis_test = [ preprocess(e) for e in sentiment_analysis_test]\n",
        "print(sentiment_analysis_test[:10])\n",
        "print(len(sentiment_analysis_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffKhách sạn tiêu chuẩn , nhưng phục vụ kém . Điểm hình tổ chức tiệc buffet đêm giao thừa , lượng người không quá đông , nhưng khâu chuẩn bị lủng củng , nhân viên chưa niềm nở với khách , thức ăn không được tươi ngon , phục vụ không đúng như thực đơn buffet ( nói tổng thể là khách sạn tiêu chuẩn phục vụ không chuẩn ) . Phục vụ và ăn uống', 'Khách sạn nằm ở trung tâm , tiện đi lại , chỉ _digitp đến mặt đường Trần Phú . Nhân viên phục vụ nhiệt tình thân thiện , tiện nghi khách sạn bình thường , mới hoạt động nên còn thiếu nhiều thiết bị , dịch vụ . Đánh giá khách sạn', 'Không có gì phải phàn nàn . Đánh giá', 'Phòng dơ , nhà tắm dơ , lúc mình tới là t_digit muốn upgrade phòng nhưng khách sạn nói là không còn phòng nữa . Không thích lắm chất lượng phòng ở đó , chỉ có vị trí gần biển ! Lần sau sẽ không ghé khách sạn này nữa ! . danh gia khach san victory', 'Khách sạn phục vụ chu đáo . Dịch vụ tốt . Nhân viên nhiệt tình . Đồ ăn hợp khẩu vị . Khách sạn Victory', 'Ấn tượng đầu tiên có lẽ là Khách sạn Công Đoàn Vũng Tàu tương đối lớn , xe hơi quá trời . Vào nhận phòng lúc _digith xem như được ưu ái _digit tý , tuy nhiên nhận phòng _digit_digit_digit , ngay ở giữa , có ban công nhưng mở cửa ra là _digit tấm kính lớn . Phòng ốc thì tạm ổn , tuy nhiên hơi cũ , có cảm giác không được sạch cho lắm . Nhìn chung là tạm ổn nếu ở mức giá _digit - cureency_k , ngày lễ mà cureency_k thì không đáng lắm . Khách sạn Công Đoàn Vũng Tàu vị trí rât thuận tiện , ra bãi sau chỉ cần chạy thẳng Hoàng Hoa Thám là tới , muốn đi ăn thì từ Khách sạn Công Đoàn Vũng Tàu chạy đi rất thuận tiện , gần bãi trước , vèo cái ra Quang Trung , ăn kem alibabas , chạy _digit chút là tới các nhà hàng hải sản . Thái độ của nhân viên thì tạm chấp nhận . Kỳ nghỉ lễ', 'Nhân viên thân thiệt nhiệt tình trong những ngày mình ở Khách sạn Indochine Nha Trang ( Đông Dương ) . Nói chung thì rất tốt , xứng đáng với giá tiền , gần biển , gần chợ đêm . Nhân viên lễ tân nhiệt tình . Lần sau có ra Nha Trang mình sẽ ghé khách sạn này . Cảm ơn bạn Thu Thương đã book giùm mình khách sạn này nhé ! Khách sạn tốt', 'Tôi tên Lai Quốc Huy - ở khách sạn Đông Dương trong _digit ngày _digit_digit / _digit _digit_digit / _digit _digit_digit / _digit . Đây là khách sạn _digit sao nên về cơ sở vật chất có thể gọi là tạm chấp nhận được . Tiếp tân - bảo vệ luôn tươi cười và niềm nở với khách , nên cũng thoải mái khi ở đây . Điều duy nhất khiến mình không thật sự là hài lòng chỉ đơn giản là phòng mình ở ( cụ thể là phòng _digit_digit_digit lầu _digit ) khi đóng cửa ra ngoài , khoảng nửa ngày sau quay về phòng thì có mùi ẩm mốc gây khó chịu cực kỳ ( phải mất hơn _digit_digit phút sau khi vào phòng và mở máy lạnh thì phòng mới thoáng trở lại ) . Đó là điều mình muốn góp ý cho khách sạn Indochine . Còn _digit góp ý nho nhỏ là dịch vụ thuê xe máy , các bạn nên đầu tư độ khoảng chục cái nón loại rẻ cũng được nhưng xinh xinh _digit tý để khách đội vào cũng thấy thoải mái , chứ nói thật trong những ngày ở đấy , mỗi lần đi ra đi vào = xe máy là một cái nón mẫu mới nhưng mẫu nào cũng gây khó chịu khi sử dụng cả ; ) - Đánh giá chung của mình là khách sạn Indochine ổn với tiêu chuẩn _digit sao - đồ ăn sáng vừa miệng ( hơi ít ) và thái độ phục vụ của toàn bộ nhân viên khách sạn đều chu đáo với khách hàng . Xin cảm ơn . Nhận xét khách quan và chân thật về khách sạn Indochine ( Đông Dương )', 'Tất cả mọi thứ đều rất tốt , chỉ có bãi biển không được sạch cho lắm . Cảm ơn Sand Hills Beach Resort & Spa và Chudu_digit_digit . . Sand Hills Beach Resort & Spa', 'Sand Hills Beach Resort & Spa chất lượng ổn ! Đánh giá']\n",
            "20241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjMkrldGtOBw"
      },
      "source": [
        "with open('sentiment_analysis_fasttext_test.txt', 'w') as f:\n",
        "  f.write('\\n'.join(sentiment_analysis_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9-lkVpftd9W",
        "outputId": "368773f9-e684-4391-ff11-665b7af94692"
      },
      "source": [
        "with open('/content/drive/MyDrive/KPDL_1/sentiment_analysis_train_v1.0.txt') as f:\n",
        "  sentiment_analysis_train = f.read().strip().split('\\n')\n",
        "  sentiment_analysis_train = [ line.split(' ',1) for line in sentiment_analysis_train]\n",
        "  sentiment_analysis_train = [ [lables, preprocess(descriptions)] for lables, descriptions in sentiment_analysis_train]\n",
        "  sentiment_analysis_train = [ ' '.join(e) for e in sentiment_analysis_train]\n",
        "print(sentiment_analysis_train[:2])\n",
        "with open('sentiment_analysis_fasttext_train.txt', 'w') as f:\n",
        "  f.write('\\n'.join(sentiment_analysis_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['__label__tot Good ratio price / service . Good advices for the national park and village . Tốt', '__label__trung_binh Trang thiết bị vệ sinh hơi cũ . Vệ sinh sạch sẽ , nhân viên thân thiện , địa điểm tốt . Một khách sạn sạch sẽ , thoải mái , dễ chịu , địa điểm tốt .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLMuJH9atond",
        "outputId": "2299f152-07d4-45fc-fed9-dfc40bed4232"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 10.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3043060 sha256=465d0876644a38ef20ae01e67062a1bfbda07b8c3481678502827094d50bb2f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMeQWUj0uTF1",
        "outputId": "3d032330-35ac-4178-81e9-612ec3dcd462"
      },
      "source": [
        "import fasttext\n",
        "\n",
        "#Train the model\n",
        "model = fasttext.train_supervised(input=\"sentiment_analysis_fasttext_train.txt\", lr=0.22, wordNgrams=3) #  epoch=5)# dim=10)\n",
        "# model.save_model(\"sell_dectection_60K_fasttext.bin\")\n",
        "prediction = []\n",
        "\n",
        "for i in range(len(sentiment_analysis_test)):\n",
        "  prediction.append(model.predict(sentiment_analysis_test[i])[0][0] )\n",
        "print(len(prediction))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DatKvvEXunZz"
      },
      "source": [
        "with open('sentiment_analysis_result.txt', 'w') as f:\n",
        "  f.write('\\n'.join(prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2f1gqMcuwME"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}